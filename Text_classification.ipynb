{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-jcOS2k7V7so"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,losses\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading movie review dataset"
      ],
      "metadata": {
        "id": "wDNvBZ1A3MRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "metadata": {
        "id": "TJpTGSGwvuUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6379f6c4-97fd-4616-b22a-2b6df2aa0897"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 5s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5N_iQUuzFck",
        "outputId": "c1dea8e1-4d40-405a-d4bf-b3727f6573c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README', 'test', 'imdb.vocab', 'train', 'imdbEr.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VokSsnndzbEW",
        "outputId": "98538872-7f72-4391-a670-809d78eb638e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unsup',\n",
              " 'pos',\n",
              " 'neg',\n",
              " 'labeledBow.feat',\n",
              " 'urls_unsup.txt',\n",
              " 'urls_neg.txt',\n",
              " 'urls_pos.txt',\n",
              " 'unsupBow.feat']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_file = os.path.join(train_dir,'pos')\n",
        "# os.listdir(pos_file)"
      ],
      "metadata": {
        "id": "HhXOOLI-zkIl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "positive review sample"
      ],
      "metadata": {
        "id": "U2HqwPC20Gd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_file = os.path.join(pos_file, '7789_10.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssBdAO9jzehy",
        "outputId": "f741e515-c161-488a-86f2-b8d02bcc59e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allison Dean's performance is what stands out in my mind watching this film. She balances out the melancholy tone of the film with an iridescent energy. I would like to see more of her.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg_file = os.path.join(train_dir,'neg')\n",
        "# os.listdir(neg_file)"
      ],
      "metadata": {
        "id": "gTGmsd0P0SSx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "negative review sample"
      ],
      "metadata": {
        "id": "J_XXYe8s0LzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_file = os.path.join(neg_file, '6064_3.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL_3lv8t0dBU",
        "outputId": "eb0ff24a-0baa-4d3d-ceb0-c837312d4d01"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well well well. As good as John Carpenter's season 1 outing in \"Masters of Horror\" was, this is the complete opposite. He certainly proved he was still a master of horror with \"Cigarette Burns\" but \"Pro-Life\" is perhaps the worst I have seen from him.<br /><br />It's stupid, totally devoid of creepy atmosphere and tension and it overstays it's welcome, despite the less-than-an-hour running time. The script is nonsense, the characters are irritable and un-appealing and the conclusion is beyond absurd.<br /><br />And for those suckers who actually bought the DVD (one of them being me); did you see how Carpenter describes the film? He's actually proud of it and he talks about it as his best work for a long time, and he praises the script. And in the commentary track, where he notices an obvious screw up that made it to the final cut, he just says he didn't feel it essential to rectify the mistake and he just let it be there. I fear the old master has completely lost his touch. I sincerely hope I'm proved wrong.<br /><br />I want to leave on a positive note and mention that the creature effects are awesome, though. Technically speaking, this film is top notch, with effective lighting schemes and make up effects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove unwanted files"
      ],
      "metadata": {
        "id": "ju5nWCHe3VMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "id": "5upId1fl1HQC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset split (training_dataset with validation split)"
      ],
      "metadata": {
        "id": "oLY5Qt1k3fCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E2EdlxA1ad-",
        "outputId": "c11c7c98-756a-42cb-b1aa-f7062b9bfb6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "few examples with label"
      ],
      "metadata": {
        "id": "3YsehQNo3rdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(2):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZHg11UR6xkI",
        "outputId": "473924cd-d8c2-48a2-e50a-8281facc423d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
            "Label 0\n",
            "Review b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
            "Label 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16yiVAww6yNX",
        "outputId": "659fca86-0daa-4d2b-b94c-01ad5a73b0be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0 corresponds to neg\n",
            "Label 1 corresponds to pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation split"
      ],
      "metadata": {
        "id": "iMEHW_dg3xXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)"
      ],
      "metadata": {
        "id": "JgJkz60Y-Fs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b7767d-3221-4d52-893e-e3ea2fcbc989"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test split"
      ],
      "metadata": {
        "id": "Dt-_04fg3zk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size)"
      ],
      "metadata": {
        "id": "RhzYM-C2oP2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946e56e7-25e6-46bc-baa7-99f478c94a4c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before using the dataset for training it needs to standardize and vectorize"
      ],
      "metadata": {
        "id": "CEzo2IZR31cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "custom standardization"
      ],
      "metadata": {
        "id": "Q1ZIJkSR4t5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation),'')"
      ],
      "metadata": {
        "id": "9VSAy106oREC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting sequence length for truncate sequence in layer"
      ],
      "metadata": {
        "id": "l4IZXAc-4MD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250"
      ],
      "metadata": {
        "id": "_11M8KsaXo0Q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorization layer"
      ],
      "metadata": {
        "id": "kVQyLE5O4ycn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize= standardization,\n",
        "    max_tokens= max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length= sequence_length)"
      ],
      "metadata": {
        "id": "egok_GCAYi2z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label is not need for vectorization"
      ],
      "metadata": {
        "id": "KHkWSusX5FE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "metadata": {
        "id": "f74mjtSbZGeL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "metadata": {
        "id": "eEwn3_1ocD4s"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[9], label_batch[9]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JYbcoRMd4Cj",
        "outputId": "b73b4ddd-2ea7-4b30-a8a5-ac0b202d91ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review tf.Tensor(b\"When I was a kid we always used to be babysat, and we always used to rent a film or see a film at the cinema. This is one of the films we watched. This is one of the stupidest films I've ever seen, I think it might even be a Walt Disney Pictures film! A martian is dropped on earth, turns into a human, befriends a human, and is trying everything he can to get back home. But he is distracted by the wonders of the Earth. The only good comment I can give is the choice of actors, Back to the Future's Christopher Lloyd as the martian, Uncle Martin, Dumb and Dumber's Jeff Daniels as Tim O'Hara, Elizabeth Hurley as Brace Channing and Daryl Hannah as Lizzie. But apart from that it's complete crap. Poor!\", shape=(), dtype=string)\n",
            "Label neg\n",
            "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
            "array([[  51,   10,   13,    4,  554,   71,  204,  330,    6,   26,    1,\n",
            "           3,   71,  204,  330,    6,  862,    4,   19,   41,   67,    4,\n",
            "          19,   31,    2,  434,   11,    7,   28,    5,    2,   94,   71,\n",
            "         289,   11,    7,   28,    5,    2, 8994,   94,  195,  121,  105,\n",
            "          10,  102,    9,  227,   53,   26,    4, 6154,  883, 1213,   19,\n",
            "           4, 7213,    7, 3357,   20,  680,  487,   81,    4,  390, 5478,\n",
            "           4,  390,    3,    7,  258,  276,   27,   68,    6,   75,  142,\n",
            "         345,   18,   27,    7, 7241,   32,    2, 3536,    5,    2,  680,\n",
            "           2,   61,   49,  902,   10,   68,  193,    7,    2, 1074,    5,\n",
            "         151,  142,    6,    2,    1, 1332, 3213,   14,    2, 7213, 1599,\n",
            "        1558,  967,    3,    1, 2008, 4740,   14, 1670, 7071, 2722,    1,\n",
            "          14,    1,    1,    3,    1, 7789,   14,    1,   18,  948,   35,\n",
            "          12,   29,  555,  591,  342,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"43 ---> \",vectorize_layer.get_vocabulary()[43])\n",
        "print(\" 907 ---> \",vectorize_layer.get_vocabulary()[907])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydM2XXeleHoc",
        "outputId": "7608b067-8fa2-4286-a71b-071480b22fdb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43 --->  has\n",
            " 907 --->  brings\n",
            "Vocabulary size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizing the dataset"
      ],
      "metadata": {
        "id": "HG4KfoSU5L_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "metadata": {
        "id": "viYiViZuflpD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset performance\n",
        "To avoid bottlenecks dataset is stored in cache after it is loaded from the disk"
      ],
      "metadata": {
        "id": "N_mWk0dv5Pn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "DhOGJniofzma"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16"
      ],
      "metadata": {
        "id": "3tXWE7Efhaww"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a neural network"
      ],
      "metadata": {
        "id": "2ym3vur05vfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc48kwWcrJep",
        "outputId": "58e9c551-2ebf-44ff-81ef-77b502c5052f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          160016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 16)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 16)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160033 (625.13 KB)\n",
            "Trainable params: 160033 (625.13 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Fk7TzAL8vHKE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk-6vv4wzjVe",
        "outputId": "94b8210b-8822-42c5-9bb4-b68558df99bf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 10s 14ms/step - loss: 0.6634 - accuracy: 0.5080 - val_loss: 0.6132 - val_accuracy: 0.5366\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.5463 - accuracy: 0.6681 - val_loss: 0.4965 - val_accuracy: 0.7318\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.4429 - accuracy: 0.7947 - val_loss: 0.4193 - val_accuracy: 0.8070\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3773 - accuracy: 0.8389 - val_loss: 0.3736 - val_accuracy: 0.8320\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3352 - accuracy: 0.8624 - val_loss: 0.3450 - val_accuracy: 0.8462\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.3045 - accuracy: 0.8769 - val_loss: 0.3259 - val_accuracy: 0.8548\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2813 - accuracy: 0.8874 - val_loss: 0.3130 - val_accuracy: 0.8616\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2613 - accuracy: 0.8966 - val_loss: 0.3035 - val_accuracy: 0.8688\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2459 - accuracy: 0.9043 - val_loss: 0.2968 - val_accuracy: 0.8702\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2303 - accuracy: 0.9094 - val_loss: 0.2924 - val_accuracy: 0.8720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the model"
      ],
      "metadata": {
        "id": "Cpp2CY8L-Q3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnIcovI8z9UU",
        "outputId": "c986393f-f097-4f89-f686-612938711051"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 5s 6ms/step - loss: 0.3105 - accuracy: 0.8606\n",
            "Loss:  0.3105068504810333\n",
            "Accuracy:  0.8605999946594238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# history_dict = history.history\n",
        "# history_dict.keys()\n",
        "# acc = history_dict['accuracy']\n",
        "# val_acc = history_dict['val_accuracy']\n",
        "# loss = history_dict['loss']\n",
        "# val_loss = history_dict['val_loss']"
      ],
      "metadata": {
        "id": "4Vzgqs6M0iPJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# plt.plot(epochs, acc, 'ro', label='Training acc')\n",
        "# # b is for \"solid blue line\"\n",
        "# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "# plt.title('Training and validation acc')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('acc')\n",
        "# plt.legend()"
      ],
      "metadata": {
        "id": "jYUpu0Yz03Ga"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "exporting model"
      ],
      "metadata": {
        "id": "57xPcZhO-bMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqk36rWN2O60",
        "outputId": "4f5cc313-b1e9-4a06-9e41-f5a2f6550a8e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3105 - accuracy: 0.8728\n",
            "0.8728399872779846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting"
      ],
      "metadata": {
        "id": "xRZrjOTs-iO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "  \"great\",\n",
        "  \" okay\",\n",
        "  \"terrible\",\n",
        "  \"Awesome\"\n",
        "]\n",
        "\n",
        "export_model.predict(examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh7MxLdo2Q-b",
        "outputId": "50bf6599-6f9a-40a6-de83-6e31dccbc9ec"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 39ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6148526 ],\n",
              "       [0.43930304],\n",
              "       [0.35352564],\n",
              "       [0.57363814]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}